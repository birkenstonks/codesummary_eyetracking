# Code Summarization
#### Code and Documentation written by Zach Karas. Email with any questions (z.karas@vanderbilt.edu)
### This repo contains the code for running, analyzing, and visualizing the code summarization project
The task has participants read snippets of Java code while their gaze is recorded using a Tobii eye-tracker. For half of the task, participants read pre-written summaries (human-written or AI-written), and for the other half, participants write their own summaries. It is randomized whether participants read or write summaries first. Java code comes from the FIXME dataset, and the AI summaries are generated by FIXME. 

## Running the Task
The main file for running the task is server2.py. It does not rely on any esoteric packages, other than perhaps tobii_research, so the setup should *hopefully* be straightforward. The task is randomized using each participant's ID number as a random seed, which ensures the order will be the same for each ID number. The task is designed so the participant gets a break in the middle of each section and in the middle of the whole task (between reading and writing). This was designed to give the participant breaks, and give the researchers the opportunity to recalibrate the eye-tracker every 15-20 minutes. 
The task is also designed so if the internet cuts out or if the task crashes, running server2.py with the same ID number will resume at the last save point. 
Below is a list of all the data types recorded during the task:

## Data - For access, please ask Zach (z.karas@vanderbilt.edu)
* Keystrokes: All keystrokes made by participants, including timepoints and the function name. Saved as <idnum>_keystrokes.csv
</br> (e.g. 008,showLatestPlan,18418213,S,2023-02-21 16:57:42.880827)
</br>
* Task: All ratings on summary reading section and all summaries on writing section. Saved as <idnum>_task.csv
</br> (e.g. ID#,	createServerChooser, 35553791,	reading,		 ,creates the server chooser, callcon,	neutral,	s_agree,	s_disagree,	agree,	36:35.9) where columns are participant_id, function_name, function_id, task, participant_summary, given_summary, summary_author, how_accurate, missing_info, unnecessary_info, end_time. For the writing condition, the ratings columns will be empty and the participant's summary will be recorded.
</br>
* Reading Save: The participant's progress on the reading task (e.g. how many stimuli they've seen)
</br>
* Writing Save: The participant's progress on the writing task (same as reading)
</br>
* Gaze folder: contains all the gaze files if eye-tracking is being recorded, split by file name. The column headers here are ['participant_id', 'function_name', 'function_id', 'system_timestamp', 'device_timestamp', 'valid_gaze_left', 'valid_gaze_right', 'gaze_left_eye', 'gaze_right_eye', 'valid_pd_left', 'valid_pd_right', 'gaze_left', 'gaze_right', 'irl_left_coord, irl_right_coord, irl_left_point_on_screen, irl_right_point_on_screen]
The irl points are the participants locations in physical space (used for creating fixation filter)

## Actual task design
The code responsible for the task is in the static/css folder and the templates folder. The templates folder contains html files corresponding to the task (reading template, writing template, rest slide template, etc.)

## Stimuli
This folder contains the "database." Just spreadsheets that store the Java functions, their id numbers, and summaries (human and AI written). 
* stimulus_selection.ipynb - takes the pruned_seeds2.csv and generates writing stimuli and reading stimuli, which don't share any functions. The task is designed so 60% of the functions seen by each participant are the same, with the other 40% being from a larger, random pool of stimuli. Here it's designed so there are 40 reading stimuli and 25 writing stimuli. This file also does a sort of bit flip for whether participants see the human-written or AI-written summaries. Since it's all randomized, this may not do very much.

## Analysis
Still a work in progress
* noise.py - crude metric for calculating noise. Basically looks at the ratio of NaN rows  when the eye-tracker couldn't record someone's eyes, and valid coordinates.  

## Bounding Boxes
This directory contains the code for creating the bounding boxes, intermediate data, and final bounding box files. The important folders are final_stimuli (images of each Java function) and word_coordinates_final (polished bounding boxes for each function). Below is a short description of each file:
* 1_draw_boxes.ipynb - this file takes the raw images from the final_stimuli folder and calculates the coordinates for the bounding boxes for each word, and symbol unfortunately. It saves a png for each word in a folder with the function name, and a word_coordinates file with each word's coordinates. Uses OCR to predict what word is in each word image. 
* 2_match_code.py - The above process isn't perfect, so I use spell check to get closer to the real words
* 3_bounding_boxes_preprocessing.ipynb - This file cleans all the csv files and removes trailing characters like ";" and boxes for paretheses only. Stores these polished files in word_coordinates_final
* localize_gaze.ipynb - Reads in the eye-tracking files and annotates each gaze point with the bounding box for the code the participant was looking at. Averages a participants left and right gaze coordinates.


## Visualization
This folder takes either a screen recording or screenshot of a function, and plots a participant's gaze coordinates. Helpful for a sanity check. The actual output I generated is stored away because it bloated the repo. This code is from another repo (https://github.com/XiangGuo1992/ORCL_VR_EyeTracking.git), so I don't know exactly how it works. It also uses a lot of labmda functions.
The general workflow is:
* 1_video_to_img.py - the file splits a screen recording into png images.  
* 2_plot_eyetracking.py - Takes the video frames and plots eye-tracking coordinates onto each image from eye-tracking files
* 3_img_to_video.py - converts images back into a video
* plot_eyetracking_scrap.ipynb - scrap file where I tried out different code

