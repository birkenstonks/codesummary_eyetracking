{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statistics import mean\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"Summary_Ratings_A1.pkl\", \"rb\") as f:\n",
    "#     rater1_ratings = pickle.load(f)   \n",
    "# with open(\"Summary_Ratings_A2.pkl\", \"rb\") as f:\n",
    "#     rater2_ratings = pickle.load(f)\n",
    "# with open(\"midprocessing/filtered_rating_neg3.pkl\", \"rb\") as f:\n",
    "#     negatives = pickle.load(f)\n",
    "\n",
    "with open(\"./midprocessing/Revised_Ratings.pkl\", \"rb\") as f:\n",
    "    ratings = pickle.load(f)\n",
    "to_toss = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def valence(num):\n",
    "#     if num > 3:\n",
    "#         return 1\n",
    "#     elif num == 3:\n",
    "#         return 0\n",
    "#     elif num < 3:\n",
    "#         return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'testNegativeParseCases': [313],\n",
       " 'setGenJarDir': [314],\n",
       " 'test': [312],\n",
       " 'updateGain': [311],\n",
       " 'appendDeclarations': [311],\n",
       " 'getUserNameFromCookie': [168],\n",
       " 'add': [186],\n",
       " 'updateSchema': [106],\n",
       " 'exportXVRL': [133],\n",
       " 'isInvalidEmailLink': [189],\n",
       " 'testOneTwoThreeCreateCycle': [155],\n",
       " 'addPKColumn': [310],\n",
       " 'equals': [310],\n",
       " 'getImageWithSource': [310],\n",
       " 'swapItems': [143],\n",
       " 'asMap': [143, 314, 136, 316],\n",
       " 'testAddCountryWithSequenceGenerator': [168,\n",
       "  133,\n",
       "  129,\n",
       "  315,\n",
       "  314,\n",
       "  117,\n",
       "  191,\n",
       "  136,\n",
       "  139,\n",
       "  317,\n",
       "  147,\n",
       "  182,\n",
       "  176],\n",
       " 'split': [133],\n",
       " 'configBalanceRanking': [314, 106, 319, 176]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter(person, func, ratings_list):\n",
    "    global to_toss\n",
    "    name = func.split('_')[0] # just the function name, without the ID number\n",
    "    try:\n",
    "        acc = int(ratings_list[0])\n",
    "        mis = int(ratings_list[1])\n",
    "        unn = int(ratings_list[2])\n",
    "        rea = int(ratings_list[3])\n",
    "    except:\n",
    "        print(person, func)\n",
    "    if mean([acc, rea]) <= 2 and mean([mis, unn]) >= 4:\n",
    "        if name in to_toss:\n",
    "            to_toss[name].append(int(person))\n",
    "        else:\n",
    "            to_toss[name] = [int(person)]\n",
    "        print(\"delete data for: \", person, func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Manual override for these summaries. They should be included\n",
    "ratings['147']['add_4114383'][3] = '3'\n",
    "ratings['147']['getNAGString_43137003'][3] = '3'\n",
    "ratings['147']['actionPerformed_12725774'][3] = '3'\n",
    "ratings['147']['testSetWelcomeMsg_33719869'][3] = '3'\n",
    "ratings['168']['appendDeclarations_36405409'][3] = '3'\n",
    "ratings['133']['equals_10222579'][3] = '3'\n",
    "ratings['311']['actionPerformed_12725774'][3]= '3'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'testNegativeParseCases': [313, 147, 319, 313],\n",
       " 'setGenJarDir': [314, 314],\n",
       " 'test': [312, 312],\n",
       " 'updateGain': [311, 191, 311],\n",
       " 'appendDeclarations': [311, 168, 311],\n",
       " 'getUserNameFromCookie': [168, 168],\n",
       " 'add': [186, 186],\n",
       " 'updateSchema': [106, 106],\n",
       " 'exportXVRL': [133, 133],\n",
       " 'isInvalidEmailLink': [189, 189],\n",
       " 'testOneTwoThreeCreateCycle': [155, 155],\n",
       " 'addPKColumn': [310, 310],\n",
       " 'equals': [310, 310],\n",
       " 'getImageWithSource': [310, 310],\n",
       " 'swapItems': [143, 143],\n",
       " 'asMap': [143, 314, 136, 316],\n",
       " 'testAddCountryWithSequenceGenerator': [168,\n",
       "  133,\n",
       "  129,\n",
       "  315,\n",
       "  314,\n",
       "  117,\n",
       "  191,\n",
       "  136,\n",
       "  139,\n",
       "  317,\n",
       "  147,\n",
       "  182,\n",
       "  176],\n",
       " 'split': [133],\n",
       " 'configBalanceRanking': [314, 106, 319, 176]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open(\"./midprocessing/Revised_Ratings.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(ratings, f)\n",
    "# to_toss = {}\n",
    "to_toss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete data for:  147 testNegativeParseCases_31696447\n",
      "delete data for:  314 setGenJarDir_31789275\n",
      "delete data for:  319 testNegativeParseCases_31696447\n",
      "delete data for:  312 test_10_bug2689872_19498280\n",
      "delete data for:  313 testNegativeParseCases_31696447\n",
      "delete data for:  191 updateGain_26215341\n",
      "delete data for:  168 appendDeclarations_36405409\n",
      "delete data for:  168 getUserNameFromCookie_1694531\n",
      "delete data for:  186 add_4114383\n",
      "delete data for:  106 updateSchema_4627680\n",
      "delete data for:  133 exportXVRL_45047585\n",
      "delete data for:  189 isInvalidEmailLink_49250848\n",
      "delete data for:  155 testOneTwoThreeCreateCycle_19507735\n",
      "delete data for:  311 appendDeclarations_36405409\n",
      "delete data for:  311 updateGain_26215341\n",
      "delete data for:  310 addPKColumn_51577053\n",
      "delete data for:  310 equals_10222579\n",
      "delete data for:  310 getImageWithSource_1782360\n",
      "delete data for:  143 swapItems_36634895\n"
     ]
    }
   ],
   "source": [
    "for person, functions in ratings.items():\n",
    "    for func, rate_list in functions.items():\n",
    "        filter(person, func, rate_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 0\n"
     ]
    }
   ],
   "source": [
    "r1_positive_ratings = []\n",
    "r1_negative_ratings = []\n",
    "r2_positive_ratings = []\n",
    "r2_negative_ratings = []\n",
    "\n",
    "r1_accuracy = []\n",
    "r1_missing = []\n",
    "r1_unnecessary = []\n",
    "r1_readable = []\n",
    "r2_accuracy = []\n",
    "r2_missing = []\n",
    "r2_unnecessary = []\n",
    "r2_readable = []\n",
    "\n",
    "\n",
    "accuracy_differences = []\n",
    "missing_differences = []\n",
    "unnecessary_differences = []\n",
    "readable_differences = []\n",
    "\n",
    "to_toss = {}\n",
    "\n",
    "for person, functions in rater1_ratings.items():\n",
    "    for func, ratings in functions.items():\n",
    "        name = func.split('_')[0]\n",
    "        other_ratings = rater2_ratings[person][func]\n",
    "        \n",
    "        ratings = [int(r) for r in ratings if r != '']\n",
    "        other_ratings = [int(r) for r in other_ratings if r != '']\n",
    "        if len(ratings) != 4 or len(other_ratings) != 4:\n",
    "            continue\n",
    "        try:\n",
    "            r1_acc, r1_mis, r1_unn, r1_rea = valence(ratings[0]), valence(ratings[1]), valence(ratings[2]), valence(ratings[3])\n",
    "            r2_acc, r2_mis, r2_unn, r2_rea = valence(other_ratings[0]), valence(other_ratings[1]), valence(other_ratings[2]), valence(other_ratings[3])\n",
    "            \n",
    "            if r1_acc != 0 and r2_acc != 0:\n",
    "                r1_accuracy.append(r1_acc)\n",
    "                r2_accuracy.append(r2_acc)\n",
    "            \n",
    "            if r1_mis != 0 and r2_mis != 0:\n",
    "                r1_missing.append(r1_mis)\n",
    "                r2_missing.append(r2_mis)\n",
    "            \n",
    "            if r1_unn != 0 and r2_unn != 0:\n",
    "                r1_unnecessary.append(r1_unn)\n",
    "                r2_unnecessary.append(r2_unn)\n",
    "            \n",
    "            if r1_rea != 0 and r2_rea != 0:\n",
    "                r1_readable.append(r1_rea)\n",
    "                r2_readable.append(r2_rea)\n",
    "            # acc_dif = abs(r1_acc - r2_acc)\n",
    "            # mis_dif = abs(r1_mis - r2_mis)\n",
    "            # unn_dif = abs(r1_unn - r2_unn)\n",
    "            # rea_dif = abs(r1_rea - r2_rea)\n",
    "            \n",
    "            # accuracy_differences.append(acc_dif)\n",
    "            # missing_differences.append(mis_dif)\n",
    "            # unnecessary_differences.append(unn_dif)\n",
    "            # readable_differences.append(rea_dif)\n",
    "            \n",
    "            # r1_pos = valence(mean([r1_acc, r1_rea]))\n",
    "            # r1_neg = valence(mean([r1_mis, r1_unn]))\n",
    "            # r2_pos = valence(mean([r2_acc, r2_rea]))\n",
    "            # r2_neg = valence(mean([r2_mis, r2_unn]))\n",
    "            \n",
    "            # r1_positive_ratings.append(r1_pos)\n",
    "            # r1_negative_ratings.append(r1_neg)\n",
    "            # r2_positive_ratings.append(r2_pos)\n",
    "            # r2_negative_ratings.append(r2_neg)\n",
    "            \n",
    "            \n",
    "            \n",
    "            if mean([r1_acc, r1_rea, r2_acc, r2_rea]) <= 2 and mean([r1_mis, r1_unn, r2_mis, r2_unn]) >= 4:\n",
    "                if func in to_toss:\n",
    "                    to_toss[name].append(int(person))\n",
    "                else:\n",
    "                    to_toss[name] = [int(person)]\n",
    "                print(\"delete data for: \", person, func)\n",
    "            # r1_positive_ratings.append(math.ceil(r1_pos))\n",
    "            # r1_negative_ratings.append(math.ceil(r1_neg))\n",
    "            # r2_positive_ratings.append(math.ceil(r2_pos))\n",
    "            # r2_negative_ratings.append(math.ceil(r2_neg))\n",
    "            # r1_positive_ratings.append(ratings[0])\n",
    "            # r2_positive_ratings.append(other_ratings[0])\n",
    "        except:\n",
    "            print(func)\n",
    "            print(ratings)\n",
    "            print(other_ratings)\n",
    "            \n",
    "print(len(r1_negative_ratings), len(r2_negative_ratings))\n",
    "print(len(r1_positive_ratings), len(r2_positive_ratings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reading_toss = pd.read_csv(\"toss_these_summaries.csv\")\n",
    "superfixpath = \"abstract_fixation_counts\"\n",
    "superdurpath = \"abstract_fixation_durations\"\n",
    "# whichtask = os.listdir(superfixpath)\n",
    "test = []\n",
    "pathname = \"reading_ratings\"\n",
    "datadir = os.listdir(pathname)\n",
    "for file in datadir:\n",
    "    name = re.sub(\".csv\", \"\", file)\n",
    "    if name in list(reading_toss['function_name']):\n",
    "        fidx = list(np.where(reading_toss['function_name'] == name))[0][0]\n",
    "        author = reading_toss.loc[fidx, 'author']\n",
    "        who_saw = pd.read_csv(f\"{pathname}/{file}\")\n",
    "        delete_idx = list(np.where(who_saw['summary_author'] == author))[0]\n",
    "        temp = list(who_saw.loc[delete_idx, 'pid'])\n",
    "        for person in temp:\n",
    "            if name in to_toss:\n",
    "                to_toss[name].append(int(person))\n",
    "            else:\n",
    "                to_toss[name] = [int(person)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"midprocessing/to_toss.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(to_toss, f)\n",
    "# iterate through data to toss\n",
    "# find "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['createCacheFile.csv', 'getBackCommand22.csv', 'getNAGString.csv', 'getBackCommand2.csv', 'getRemoteObject.csv', 'setPhoto.csv', 'testValidateSeparationCount.csv', 'saveToFile.csv', 'testSetWelcomeMsg.csv', 'testMoveRFWithNullContainer.csv', 'compareTo.csv', 'makeUniqueParagraphForGlobalWithLength.csv', 'populateNewList.csv', 'visitRetStmt.csv', 'saveSetting.csv', 'clearFieldersFromField.csv', 'goToRegistration.csv', 'getClassNameForLookAndFeel.csv', 'listen.csv', 'testSetExample.csv', 'testCookieGreaterThan.csv', 'showLatestPlan.csv', 'messageSent.csv', 'testGetEmail.csv', 'handleHalt.csv', 'go.csv', 'abstractMatrix3D.csv', 'deleteCascade.csv', 'selectBracketingEntries.csv', 'invalidateSession.csv', 'onAttach.csv', 'getEffect.csv', 'getTargetServiceName.csv', 'setSecurityMode.csv', 'play.csv', 'getMenuAdministracion.csv', 'searchRecipe.csv', 'removeService.csv', 'getJSplitPane.csv', 'addRotation.csv', 'setUpMrj.csv', 'testInvoke.csv', 'store.csv', 'readFromFile.csv', 'evaluate.csv', 'getApplicableLaw.csv', 'BFSdist.csv', 'createChecklistItem.csv', 'actionPerformed.csv', 'getRodinDBStatus.csv', 'toString.csv', 'liesBetween.csv']\n"
     ]
    }
   ],
   "source": [
    "print(writefiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testNegativeParseCases\n",
      "setGenJarDir\n",
      "test\n",
      "updateGain\n",
      "appendDeclarations\n",
      "getUserNameFromCookie\n",
      "add\n",
      "updateSchema\n",
      "exportXVRL\n",
      "isInvalidEmailLink\n",
      "testOneTwoThreeCreateCycle\n",
      "addPKColumn\n",
      "equals\n",
      "getImageWithSource\n",
      "swapItems\n",
      "asMap\n",
      "testAddCountryWithSequenceGenerator\n",
      "split\n",
      "configBalanceRanking\n"
     ]
    }
   ],
   "source": [
    "readfiles = os.listdir(\"abstract_fixation_counts/reading\")\n",
    "writefiles = os.listdir(\"abstract_fixation_counts/writing\")\n",
    "for k, v in to_toss.items():\n",
    "    print(k)\n",
    "    filename = k+\".csv\"\n",
    "    if filename in readfiles:\n",
    "        task = \"reading\"\n",
    "    elif filename in writefiles:\n",
    "        task = \"writing\"\n",
    "    fdf = pd.read_csv(f\"abstract_fixation_counts/{task}/{filename}\")\n",
    "    ddf = pd.read_csv(f\"abstract_fixation_durations/{task}/{filename}\")\n",
    "    fmask = ~fdf['pid'].isin(v)\n",
    "    dmask = ~ddf['pid'].isin(v)\n",
    "    fdf = fdf[fmask]\n",
    "    ddf = ddf[dmask]\n",
    "    # os.system(f\"mv abstract_fixation_counts/{task}/{filename} unfiltered_backup/fc_{filename}\")\n",
    "    # os.system(f\"mv abstract_fixation_durations/{task}/{filename} unfiltered_backup/fd_{filename}\")\n",
    "    # fdf.to_csv(f\"abstract_fixation_counts/{task}/{filename}\", index=False)\n",
    "    # ddf.to_csv(f\"abstract_fixation_durations/{task}/{filename}\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readfiles = os.listdir(\"abstract_fixation_counts/reading\")\n",
    "writefiles = os.listdir(\"abstract_fixation_counts/writing\")\n",
    "for files in to_toss.items():\n",
    "    print(k)\n",
    "    filename = k+\".csv\"\n",
    "    if filename in readfiles:\n",
    "        task = \"reading\"\n",
    "    elif filename in writefiles:\n",
    "        task = \"writing\"\n",
    "    df = pd.read_csv(f\"abstract_fixation_counts/{task}/{filename}\")\n",
    "    # ddf = pd.read_csv(f\"abstract_fixation_durations/{task}/{filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "661 data points in the writing condition\n",
      "996 data points in the reading condition\n"
     ]
    }
   ],
   "source": [
    "which_task = os.listdir(\"abstract_fixation_counts\")\n",
    "for task in which_task:\n",
    "    files = os.listdir(f\"abstract_fixation_counts/{task}\")\n",
    "    data_points = 0\n",
    "    for file in files:\n",
    "        df = pd.read_csv(f\"abstract_fixation_counts/{task}/{file}\")\n",
    "        data_points += len(df)\n",
    "    print(f\"{data_points} data points in the {task} condition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
